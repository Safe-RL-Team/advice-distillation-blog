<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/front-matter">
  title: "Teachable Reinforcement Learning via Advice Distillation"
  description: "Description of the post"
  authors:
  - Claire, Sturgil:
  - Mihai, Dumitrescu: https://github.com/midumitrescu
  affiliations:
  - Google Brain: http://g.co/brain
  - BCCN Berlin: https://www.bccn-berlin.de/
</script>

<dt-article>
  <h1>Teachable Reinforcement Learning via Advice Distillation</h1>
  <h2>A description of the article</h2>
  <dt-byline></dt-byline>

  <h1>Abstract 1</h1>
  <p>This is the first paragraph of the article.</p>
  <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>

  <h1>Introduction</h1>
  <p>Text for paragraph 2</p>

  <h1>Background</h1>
  <p>Text for paragraph 3</p>

  <h2>Problem setting</h2>
  <p>Text for paragraph 3</p>

  <h1>Method</h1>
  <p>Text for paragraph 3</p>

  <h1>Experimental setup</h1>

  <p>To test the paper's approach, we compared the method of advice distillation described above with a simple baseline case: training a multi-layer perceptron (mlp) to convert high-level to low-level advice. The basic steps for our method are:</p>

  <ol>
    <li>Train an mlp to take high-level advice as input and return eqivalent low-level advice</li>
    <li>For the grounding phase, train our agent on low-level advice just like in the paper's method</li>
    <li>For the distillation phase, keep the agent the same and replace the low-level advice with the mlp's output</li>
  </ol>
  
  <p>With this approach, the agent does not have to learn how to learn how to follow a new kind of advice because the advice it gets is equivalent to what it was getting before. Instead, the training time is taken in advance by pre-training the mlp.</p>

  <p>We chose this baseline of comparison because the goal of advice distillation is to transfer the knowledge learned from low-level advice in the grounding phase to higher-level advice. As proposed in the original paper and supported by their experiments, this allows the agent to learn faster (both in terms of literal training time and the amount of instruction needed) than it does if it starts with only the high-level advice.</p>

  <p>Our advice-conversion mlp applies the same principle with a very basic architecture, directly mapping high-level onto low-level advice instead of training the agent to follow the high-level advice directly. By comparing the paper's method against this baseline, we can test whether training the agent to actually act on high-level advice results in better performance, or if a direct advice-mapping is sufficient.</p>

  <p>Our advice-conversion mlp had a 383-value input layer, consisting of a 255-value observation of the environment state and a 128-value offset waypoint advice component, a 128-value hidden layer, and a 2-value output layer. The training set consisted of observation/offset waypoint advice/direction advice triples. For each triple the waypoint location and agent position and velocity were randomly generated, and the agent's usual offset waypoint and direction teachers were queried to get the input and label respectively. The offset waypoint advice was passed through a fixed-weight mlp to expand it to 128 dimentions before being passed as input to the advice-conversion mlp</p>

  <p>Because the high-level advice is sparse, the agent may have moved since the last offset waypoint was given, so the correct direction that would be given as low-level advice may not be the same direction given by the old high-level advice. To simulate this, we included each generated waypoint five times in the training set, each with a different random nearby agent position. The offset waypoint advice given was always based on the first position in the set, but the directional advice label was based on the actual current position. This ensured that the agent would not just copy the offset waypoint given but would take the actual state of the environment into account as well.</p>

  <p>One weakness of this training set is that it does not have any memory of previous environmental states. In our input generation the agent positions are independent of each other, but in an actual environment the next position would be based on the current position and velocity and the action taken. We did not include this because we wanted to keep our architecture simple to serve as a more effective baseline. But having an understanding of previous states and actions is one advantage the advice distillation agent has over this baseline. Future experiments could expand this mlp to take this information into account, for example changing the output to a time series representing the best actions to take over several time steps to reach the waypoint.</p>

  <p>The advice converter was trained using stochastic gradient descent with 5000 batches of 10,000 values each. The step size was initially 0.001 and was reduced to 0.0001 after 100 epochs. After a training time of about 7 hours, the mlp achieved a final loss of about 2.47. See the results section for a more detailed analysis of the training loss and its implications for the mlp's performance. During the distillation phase, the offset waypoint advice that would normally be passed directly to the agent was instead run through this mlp, and the mlp's output was passed to the agent instead.</p>

  <p>For our experiment, both our method and the paper's method used the same grounded agent, which was run for 320 iterations on directional advice. For the paper's method, a new agent that took offset waypoint advice was created, and the old policy was used for off-policy relabeling. For our method, the same agent was kept and the directional advice was replaced with the output of our pretrained advice-conversion mlp. Our method was run for 900 more iterations, and the paper's method was only run for 440 more iterations before an issue caused the training to stop early. As a result, we focused on the first 440 post-grounding iterations for our analysis.</p>

  <h1>Results and Discussion</h1>

  <p>We measured the loss (mean squared error) of our advice-conversion mlp during pretraining as a measure of how well it could approximate the real directional advice.</p>

  <img src="https://cdn.discordapp.com/attachments/875837432870879295/1090266920198086716/mlp-loss.png">

  <p>Note that since a brand new batch of training data is generated at every epoch, there isn't a test/training loss split in this case because every input is one the network has not seen before. At the end of training, the network's loss was around 2.5. The values in the direction vector were restricted to a range of -3.8 to 3.8, so a loss of 2.5 means the network's output is still relatively inaccurate.</p>

  <p>This seems to be because of the number of weights involved in the network. An earlier version of the advice-training took a 6-value input (just the offset waypoint advice, the agent position, and the agent velocity) and achieved a much better performance, with a loss of 0.5 after only an hour of training. However, this model only worked when the offset waypoint advice was given densely, because it has no way to know the true waypoint location if the offset waypoint given is inaccurate. The current advice converter, while much slower to converge, is able to properly interpret sparsely given advice.</p>

  <p>While we did not have time to train the mlp for longer, its performance was still improving at the end of the pretraining period, so it would likely continue to improve with more training time. Future experiments could confirm this by testing the effects of varying training times, and the effect of a better-converged model on the agent's overall performance.</p>

  <p>We also compared the average reward of the agents using both the paper's method and ours as a measurement of how well the agents were able to complete the task.</p>

  <img src="https://raw.githubusercontent.com/midumitrescu/teachable-rl/main/graph.png">

  <p>Some of the specific reward values are highlighted in the table below:</p>

  <p></p>

  <table>
    <tr>
      <th>Iteration</th>
      <th>Original Distillation Reward</th>
      <th>Our Method Reward</th>
    </tr>
    <tr>
      <td></td>
      <td><b>Grounding Phase (same agent for both methods)</b></td>
      <td></td>
    </tr>
    <tr>
      <td>0</td>
      <td>0.00266</td>
      <td>0.00266</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.06159</td>
      <td>0.06159</td>
    </tr>
    <tr>
      <td>200</td>
      <td>0.07142</td>
      <td>0.07142</td>
    </tr>
    <tr>
      <td>300</td>
      <td>0.08013</td>
      <td>0.08013</td>
    </tr>
    <tr>
      <td></td>
      <td><b>Start of Distillation (at iteration 320)</b></td>
      <td></td>
    </tr>
    <tr>
      <td>320</td>
      <td>0.00533</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>420</td>
      <td>0.00416</td>
      <td>0.01886</td>
    </tr>
    <tr>
      <td>520</td>
      <td>0.00466</td>
      <td>0.02290</td>
    </tr>
    <tr>
      <td>620</td>
      <td>0.002</td>
      <td>0.02207</td>
    </tr>
    <tr>
      <td>720</td>
      <td>0.00666</td>
      <td>0.03841</td>
    </tr>
  </table>

  <p>The agent improves rapidly during the grounding phase, as it it given relatively simple and informative directional advice to follow. When the distillation phase begins, both agents performance drops sharply. However, while the original distillation agent's performance is as poor as it was at the beginning, the agent using our method is still somewhat better.</p>

  <p>This is consistent with what we would expect given how the agents work. The paper's version of the distillation starts with a new agent and newly-collected trajectories, so it essentially has to restart its learning from scratch. Our version, however, keeps the same agent and just changes to a new advice type that is trained to match the old advice type, so it retains some of its progress. As mentioned above, our mlp was still a relatively inaccurate approximation of the actual low-level advice, explaining the drop that we do see. Presumably, if the mlp was allowed to train longer, this drop would be smaller because the network's output would be closer to the accurate directional advice that the agent is used to recieving. As the new advice types are not as easy to interpret as the old directional advice, the agents do not improve as quickly in the distillation phase, but we would expect to see them converge to a better performance given more iterations to run.</p>

  <p>Our method's ability to switch to a higher-level advice type without a drop in performance may be useful in situations where a smooth transition between advice types is necessary. However, because the mlp's output will always be at least somewhat different from the actual best action we suspect that the original advice distillation method will eventually converge to a better performance. While our method accomplishes the basic goal of allowing an agent trained on low-level advice to understand high-level advice using only a simple mlp architecture, more distillation iterations would be needed to compare the long-term performance of the two methods.</p>

  <h1>Conclusion</h1>

  <p>The point-maze agent learns quickly when given low-level directional advice, but its performance drops when switching to high-level offset waypoint advice at the start of the distillation phase. Because the new advice is an approximation of the old directional advice, our advice-conversion method experiences less of a drop and initially performs better than the paper's advice-distillation method.</p>

  <p><b>Limitations</b></p>
  <p>The main limitation of our experiment is the limited time we were able to run the grounding, distillation and pre-training processes for. Because of this, we chose to focus on the immediate consequences of the switch in advice type, but we would need more time for an effective comparison of the two methods' convergence speeds or overall performance. More pre-training time would also likely improve the performance of our method's agent because the advice it is receiving would be closer to the directional advice it was grounded on.</p>

  <p>Additionally, our method has some limits that would make it impractical for some cases. First, it needs a large amount of high- and low-level advice to serve as a training dataset, which could be impractical for some cases, for instance if the advice needs to be human-provided. Allowing the advice-conversion mlp to continue to train during the agent's own training would help with this issue, but then would not allow as smooth of a transition as the pre-training does. The pairing of high-level with low-level advice in pairs also would not work in cases where the two advice types do not have a clear relationship. Finally, the methods used to generate the mlp's training data may not reflect the actual environmental conditions (for instance, our training data assumed no walls inside of the maze), which may hurt the agent's performance when using this data.</p>

  <p><b>Future Research Directions</b></p>
  <p>Simply running the phases of our method for longer would be a good future experiment, allowing the later performance of the two methods to be compared. There are also several tweaks to our method that could be tested in future experiments. The time-convergence tradeoff of the advice-conversion mlp could be explored, and the mlp could be allowed to continue training during the distillation phase or even integrated into the agent's own network rather than being separate. Finally, both the paper's and our methods could be applied to other types of environments and advice, in order to see if the results hold across environment and advice types.</p>


</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
</script>

<style>
  table, th, td {
    border: 1px solid;
  }
</style>