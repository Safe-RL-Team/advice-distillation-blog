<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/front-matter">
  title: "Teachable Reinforcement Learning via Advice Distillation"
  description: "Description of the post"
  authors:
  - Claire, Sturgil:
  - Mihai, Dumitrescu: https://github.com/midumitrescu
  affiliations:
  - Google Brain: http://g.co/brain
  - BCCN Berlin: https://www.bccn-berlin.de/

















</script>

<dt-article>
    <h1>Teachable Reinforcement Learning via Advice Distillation</h1>
    <h2>A description of the article</h2>
    <dt-byline></dt-byline>

    <h1>Abstract 1</h1>
    <p>This is the first paragraph of the article.</p>
    <p>We can also cite
        <dt-cite key="gregor2015draw"></dt-cite>
        external publications.
    </p>

    <h1>Introduction</h1>

    <p><b>
        <dt-cite key="suttonbarto2020">Reinforcement Learning</dt-cite>
    </b>
        is a machine learning technology in which agents explore their environments, receive reward and
        learn strategies for maximizing the amount of received reward.
    </p>

    <p>The typical way in which agents learn is trough exploration of the environment and by receiving rewards again by the environment.
        Agents have to <i>"try out"</i> various actions in each state by
        <i>"choosing"</i> from a set of possible
        actions. Normally agents are learning from scratch and without any <i>"guidance"</i>.
        Thus, they must try out particular combinations <b>(state, action)</b> pairs to
        make sure they have exhaustively found out <i>"enough"</i> information about the environment.
    </p>

    <p>RL has been employed successfully in solving some tasks with performance better than humans have been able to do.
        However, the setting until now has been
        <dt-cite key="mnih2013">quite limited</dt-cite>
        .
        Ideally, we wish to have agents that are able to solve from complex to
        <dt-cite key="dsilver2017">very complex tasks</dt-cite>
        .
        This implies, to some extent, that the space (state, action) is of very high dimensionality. Combined
        with the
        random exploration technique, this forces the agent to do a lot of random, unguided exploration. This, in turn,
        leads to
        the
        issue of requiring
        <dt-cite key="barto90">high number of samples</dt-cite>
        . For example, in obtaining at least 20% of human performance in <i>Atari Games</i>
        the algorithms required at least
        <dt-cite key="hessel2017">10 million samples</dt-cite>
        .
    </p>

    <p>This is in stark contrast to how humans learn, though. Humans start as children
        <dt-cite key="lyoms2007">imitating</dt-cite>
        what other humans do.
        They then continue imitation by learning
        <dt-cite key="korteling2021">indirectly</dt-cite>
        using
        <dt-cite key="chopra2019">communication</dt-cite>
        in
        <dt-cite key="lynn2019">in natural language</dt-cite>
        . <dt-cite key="morgan2015">Humans communicate</dt-cite> with low effort <dt-cite key="waxman1995">high bandwidth</dt-cite>.
        In this way, they are being told how to solve tasks,
        typically by more expert peers (e.g. going to school, university or having a coach).
        During the learning process, students are receiving constant feedback from their peers on how well they do. Thus
        humans quickly calibrate to make sure that task solving strategies are appropriate.
        The experts also usually use a complex stepwise strategy for teaching. The student usually starts with some very basic
        training and gets
        introduced to more complex tasks after he has a pretty good understanding on how to solve easier tasks.
    </p>
    <p>Interestingly enough, research suggests that humans themselves are driven by inner rewards.
        One of the main neurotransmitters is
        <dt-cite key="juarez2016">dopamine</dt-cite>
        . Its purpose is to encode
        <dt-cite key="bayer2005">reward prediction error</dt-cite>
        . Work has been done suggesting dopamine is a very good candidate signal for
        <dt-cite key="schultz1997">driving learning</dt-cite>
        .
    </p>

    <p>Humans typically learn faster and require fewer samples.</p>

    <p><dt-cite key="watkins2023">The paper we investigated</dt-cite> suggests solving issue of high number of samples by enabling agents to follow a similar learning
        strategy.
        More concretely, agents are made <dt-cite key="arumugam2019">teachable</dt-cite> i.e. learn how to follow instructions
        from humans.
        The teachers are giving instructions to the agent on how to solve intermediary steps of a task and are not
        allowed to directly
        control the agent's movements. The paper calls these instructions <b>advice</b></p>

    <p>Similarly to the stepwise school curriculum of humans, the agents are trained on various levels of complexity of
        the <b>advice</b>.
        The paper suggests 3 steps of learning:
    <ol>
        <li><b>Grounding</b> - teaching the agent how to follow, simple, <i>low level <b>advice</b></i></li>
        <li><b>Grounding to multiple types of advice</b> - teaching the agent how to follow, tuples of simple, <i>low level <b>advice</b></i></li>
        <li><b>Improvement to higher level advice</b> - teaching the grounded agent to follow more complex, <i>higher
            level <b>advice</b></i></li>
        <li><b>Improvement to advice independence</b> - removing the teacher completely and allow the agent to interact
            with its environment independently
        </li>
    </ol>
    </p>

    <p>After learning, goes through a typical <b>evaluation</b> phase to test its performance.</p>

    <p>The paper makes the claim that it <i>["proposes a framework for training automated agents using similarly
        rich interactive supervision"]</i> that we do not regard as being true. The advice implemented in the codebase not
        rich at all,
        see @Method. We will discuss in the before last paragraph a suggestion on how to extend this to a more rich
        language.
    </p>
    TODO Talk about requiring less samples and also requiring less complex environments for learning.

    <p>This is achieved via augmenting the reward signal typical in RL setting. The teacher has the ability to present a
        reward to the agent depending on how well it is following the given advice. Thus, the teacher acts as a
        <dt-cite key="macglashan2017">coach</dt-cite> and the <dt-cite key="arumugam2019">agent learns how to react to human feedback</dt-cite>.</p>

    <p>To understand how this works, we will
        present the <b>Coaching-Augmented Markov Decision Process</b> formalism in the next section.

        We will then
        explain how
        this formalism is used to leverage this tiered structure of learning using
        <dt-cite key="munos2016"><b>Off-policy Learning</b></dt-cite>
        <dt-cite key="precup2001"><b>see also</b></dt-cite>
        .

        We will then present our contribution to how we made the algorithm make use of
        <d-cite key="suttonbarto2020"><b>On policy Learning</b></d-cite>
        .
        We will present some preliminary results, talk about the challenges we faced and then open up a discussions
        section.
    </p>.

    <p>Other attempts have been at enabling agents to learn <i>more like</i>humans do. This includes:
        <ul>

    <li><dt-cite key="morgan2015">imitation learning</dt-cite> i.e. <dt-cite key="ziebart2008">closely mimicking demonstrated behaviour</dt-cite></li>
            <li>No Regret Learning: <dt-cite key="">DAgger</dt-cite></li>
            <li><dt-cite key="christiano2023">Preference Learning</dt-cite></li>
        </ul>
    </p>

    <p>The big disadvantage of these techniques, though, is the low bandwidth of communication.
        This means that little <dt-cite key="knox2008">information</dt-cite>
    is extracted from each interaction with humans.</p>


    <h1>Background</h1>
    <h2>Markov Decision Processes</h2>
    <p>RL typically works by implementing the <b>Markov Decision Process</b> formalism. The MDP is defined as a tuple
        {S, A, T, R, ρ, γ, p} where
    <ol>
        <li>S is the <i>state space</i> and represents valid positions where the agent could be found at any time</li>
        <li>A(s) is the <i>action space</i> and represents the valid actions that an agent can take being in a
            particular state
        </li>
        <li>T(s<sub>t</sub>, a, s<sub>t+1</sub>) is the <i>transition dynamic</i> and represents the probability of
            arriving at
            s<sub>t+1</sub> if at time t the agent was at s<sub>t</sub> and executing action a
        </li>
        <li>R(s, a) is the <i>reward</i> that an agent receives being and state s and executing action a</li>
        <li>ρ(s<sub>0</sub>) is the <i>initial state distribution</i>representing where the agent starts each episode
        </li>
        <li>γ is the <i>discount factor</i> balancing how important future rewards vs immediate ones are</li>
        <li>p(τ) is the <i>distribution over tasks</i> i.e. what kind of task the agent is supposed to solve</li>
    </ol>
    </p>

    <p>The agent decides on an action to take at each time step <i>t</i>. A set of decisions the agent takes is called a
        <b>policy</b> and is typically denoted by <b>π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ)</b>. The policy is
        called
        <b>θ</b> and is usually implemented by a probability distribution on the set <b>A</b>.

        The agent thus interacts with the environment and collects <b>trajectories</b> of the shape</p>
    <dt-code block>
        D = {(s<sub>0</sub>,a<sub>0</sub>,r<sub>1</sub>),(s<sub>1</sub>,a<sub>1</sub>,r<sub>2</sub>),···
        ,(s<sub>H-1</sub>,a<sub>H-1</sub>,r<sub>H</sub>)}<sub>j=1</sub><sup>N</sup>.
    </dt-code>


    <h3>Solving the <b>MDP</b></h3>
    <p>The objective of a multi task <b>MDP</b> is to find the <b>policy θ<i></i></b> that maximizes the amount of
        future discounted rewards. Formally, it looks for</p>
    <p>

        max<sub>θ</sub> [<b>E</b><sub>a<sub>t</sub>∼π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ)</sub>(∑<sup>∞</sup><sub>t=0</sub>
        γ<sup>t</sup>r(s<sub>t</sub>, a<sub>t</sub>, τ)>)]

    </p>

    <p>
        where <b>E(</b>X<b>)</b>=&lt;X&gt; represents the expected value the random variable X.
    </p>

    <h3>Exploration/exploitation dilemma</h3>

    <p>Typically agents need to execute random actions to discover trajectories which prove to be of high reward. In
        case such
        are found, the agent increases the probability of taking similar actions in the future. Because of high
        dimensional <b>state x action</b> space,
        the agent typically needs to try out a lot of combinations to make sure it found the best one. The agent always
        needs a balance
        between trying out random new actions and commit to already known high reward ones. It is still an unsolved
        problem
        to find this optimal balance. This is called the <b>exploration/exploitation dilemma</b> agents typically face
        and quickly explains the need for many samples (#introduction).
    </p>


    <h2>Coaching-Augmented Markov Decision Processes</h2>

    <p>The paper extends the classical <b>MDP</b> by providing two extensions:
    <ol>
        <li>C = {c<sub>t</sub>}, the set of <i>coaching functions</i>
            where c<sub>t</sub> represents advice given to the agent at time <i>t</i>.
        </li>
        <li>R<sub>CMDP</sub>=R(s,a) + R<sub>coach</sub>(s,a), where R(s,a) is the previous reward presented by the
            environment and
            R<sub>coach</sub>(s,a) represents the added reward the coach provides if the agent follows his advice
        </li>
    </ol>
    </p>

    <p><br/>c<sub>j</sub> used in the paper is either:
    <ol>
        <li>Cardinal Advice <i>(Up (0,1), Down(0, -1), Est(-1, 0) or West(1, 0)</i></li>
        <li>Directional Advice <i>(0.5, 0.5)</i></li>
        <li>Waypoint Advice <i>(e.g. Go To (3,1))</i></li>
    </ol>
    </p>

    <p>but could be extended to include natural language richer type of advice (see #discussions)</p>

    <p>

        Like this, we formally define the <b>Coaching Augmented MDP (CAMP)</b> = {S, A, T, R<sub>CMDP</sub>, ρ, γ, p,
        C}.

        The agent then captures trajectories of the shape:
    </p>
    <dt-code block>
        <p>D = {(s<sub>0</sub>,a<sub>0</sub>,c<sub>0</sub>
            ,r<sub>1</sub>),(s<sub>1</sub>,a<sub>1</sub>,c<sub>1</sub>,r<sub>2</sub>),···
            ,(s<sub>H-1</sub>,a<sub>H-1</sub>, c<sub>H-1</sub>,r<sub>H</sub>)}<sub>j=1</sub><sup>N</sup>.</p>
    </dt-code>

    <p>
        The new optimization problem is to find the <i>best</i> policy <b>θ</b> that maximizes rewards from <b>both</b>
        the environment
        and the coaching functions i.e.
    </p>
    <p>max<sub>θ</sub> [<b>E</b><sub>a<sub>t</sub>∼π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ,
        c<sub>t</sub>)</sub>(∑<sup>∞</sup><sub>t=0</sub> γ<sup>t</sup>r(s<sub>t</sub>, a<sub>t</sub>, c<sub>t</sub>,
        τ)>)]</p>

    <p>representing an agent that interacts with the environment and has access to advice presented under the form of
        the coaching function c<sub>t</sub>.
    </p>

    <p>The big advantage of <b>CMDP</b> to plain <b>MDP</b> is that it formalizes the interaction of the agent with
        a <i>human in the loop trainer</i>. The agent learns that <i>following human instructions/advice provides
            reward</i> and
        it starts doing so.
        The agent can now take advantage of <i>expert knowledge</i>.
        TODO: this could be improved
    </p>

    <h1>Method</h1>
    <p>Our target is to quickly train agents that are able to solve complex tasks.
        Considering the #Exploration/exploitation dilemma, we would want agents that quickly find high reward
        policies eliminating a lot of random exploration.</p>
    <p>The paper suggests a tier based teaching scheme, speeding up learning
        versus typical <b>MDP</b>.
    </p>.

    <p>
        This is done by:
    <ol>
        <li>making the agent follow the coaching it receives</li>
        <li>introducing increasingly complex coaching</li>
        <li>guiding the agent to the goal</li>
        <li>allowing him to quickly understand that specific <i>policies</i>provide <b> high reward</b></li>
        <li>eliminate the coaching</li>
        <li>allow the agent to follow the already found <b>high reward</b> policies</li>
    </ol>

    <p> The paper introduces the following phases:
    <ol>
        <li><b>Grounding</b> - with the focus of making the agent interpret and follow <i>low level, simple</i> <b>advice</b>
        </li>
        <li><b>Improvement</b>, which is of two types:
            <ol>
                <li>from one type of <b>advice</b> to another type of <b>advice</b> - typically from <i>low level,
                    simple</i> <b>advice</b> to
                    <i>high level, more complex</i> <b>advice</b>
                </li>
                <li>from one type of <b>advice</b> to <b>no</b> <b>advice</b> - allowing the agent to figure out
                    policies
                    that allow him to decide independently on next actions
                </li>
            </ol>
        </li>
        <li><b>Evaluation</b> - which represents the phase in which the agent does not learn anymore and the already
            learned policy is evaluated
        </li>
    </ol>

    </p>
    <h3>Grounding</h3>
    <p>The main objective of grounding is to make the agent follow/interpret the provided <b>advice</b>.
        The big advantage vs. plain <b>MDP</b> solving tasks is that the agent can be trained on a <i>very simple</i>
        environment. The trajectories can be chosen a lot simpler than the ones in a complex environment, where the
        agent
        must follow many steps to reach a goal (e.g. a game or a maze).
    </p>
    <p>Theoretically the advice in the grounding phase can be of any nature. However, chosen wisely it can support the
        idea of tiered
        learning. Therefore, <i>the grounding</i> phase is the candidate for the simplest available advice i.e.
        <b>Directional Advice</b>
        At every time step, the agent is rewarded with the dot product between the advised direction and the action it
        took.
        E.g. Should the agent be advised to move up (i.e. Direction (0, 1)) and he moves in direction (0, 0.5) he will
        be rewarded with <(0, 1) * (0, 0.5)> = 0.5.<br/>
        Should he move in direction (1, -0.5) i.e. diagonally down, he will receive a negative reward of
        <(0, 1) * (1, -0.5)> = - 0.5
    </p>

    <p>By applying the framework of <b>CMDP</b> with the provided <i>low-level</i> advice, then we will obtain the
        grounded
        policy</p>
    <p>π<sub>θ<sub>grounded</sub></sub>(&centerdot;|s<sub>t</sub>, τ, c<sub>low level, t</sub>)</p>

    <p>i.e. a policy that can take the state s, target τ and the <i>low level</i> advice c<sub>t</sub> that provides
        a probability distribution of next actions.</p>

    TODO: talk on how advice is actually inputted in the actor mlp

    <h3><b>Distillation</b> to other types of advice</h3>
    <p>Once we have a policy able to interpret the simplest type of advice, we can use it to quicker teach the agent
        other types of advice.
    </p>
    <p>The process of using a type of advice to quicker learn another one is called <b>distillation</b> and represents
        the key innovation of this paper.</p>

    <p>Formally, the agent gathers trajectories of the shape:</p>
    <p>
        D = { (s<sub>0</sub>, a<sub>0</sub>,c<sup>l</sup><sub>0</sub>, c<sup>h</sup><sub>0</sub>, r<sub>1</sub>),
        (s<sub>1</sub>, a<sub>1</sub>,c<sup>l</sup><sub>1</sub>, c<sup>h</sup><sub>1</sub>, r<sub>2</sub>),···,
        (s<sub>H-1</sub>,a<sub>H-1</sub>, c<sup>l</sup><sub>H-1</sub>, c<sup>h</sup><sub>H-1</sub>,r<sub>H</sub>)}<sub>j=1</sub><sup>N</sup>.
    </p>

    <p>
        c<sup>l</sup><sub>t</sub> represents the <b>low level</b><i> advice</i> while c<sup>h</sup><sub>t</sub>
        represents the
        <b>high level, more complex</b> type of <i>advice</i>.
    </p>

    <p><b>Distillation</b> can be achieved using two types of learning:
    <ol>
        <li>using <i>off-policy actor critic</i> learning - done mostly codebase accompanying the paper</li>
        <li>using <i>on-policy actor critic</i>learning combined with supervised learning of the mapping from
            <i>low level <b>to</b> high level advice</i> learning - done in the code extension we implemented
        </li>
    </ol>
    </p>
    <p>In the first method, the new to learned policy π<sub>Φ<sub>new</sub></sub> is a newly initialized policy. The
        agent
        explores the environment using <b>Φ<sub>new</sub></b> but learns by using the off-policy <b>θ<sub>grounded</sub></b>.
        This approach comes with the fact that the exploration/exploitation dilemma basically is <b>reset</b>. The agent
        is forced
        to execute enough random exploration. By having enough trajectories, such that the off-policy offloading
        carries over the grounded knowledge base.
    </p>
    <p>We tried to tackle this issue in our implementation. We reused the already existing
        <b>θ<sub>grounded</sub></b> by learning a mapper from the <i>new</i> type of advice to the <i>old one</i>.
        Like this, the old policy continues to work because of no change in the structure of parameters.</p>
    <p>
        The mapping from c<sup>h</sup><sub>t</sub> to c<sup>l</sup><sub>t</sub> we learned via supervised learning.
        Our reasoning was that we can take advantage of the existing pairs (c<sup>h</sup><sub>t</sub>,
        c<sup>l</sup><sub>t</sub>)
        that can be learned in a supervised way.</p>
    <p>The expectation was then that <b>θ<sub>grounded, high level -> low level</sub></b> would start from a higher
        baseline than
        <b>Φ<sub>new</sub></b>. This should be measurable in experiments.
    </p>

    <p>The target of grounding is thus achieved by having a well grounded policy. This can accept multiple types of
        advice. </p>
    <p>π<sub>Φ</sub>(&centerdot;|s<sub>t</sub>, τ, <b><u>c<sub>t</sub></u></b>)</p>
    <p> where <b><u>c<sub>t</sub></u></b> represents the tuple
        (c<sup>l</sup><sub>t</sub>, c<sup>h<sub>1</sub></sup><sub>t</sub>, c<sup>h<sub>2</sub></sup><sub>t</sub>, ...).
    </p>

    <h3>Improvement</h3>
    <p>The ultimate goal is to obtain a policy</p>
    <p>π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ)</p>
    <p>which does not require the coaching functions. The paper uses the already explained <b>distillation</b> technique
        to learn such a policy.</p>.
    <p>
        Distillation can be done either:
    <ol>
        <li>by distilling from the grounded policy to another intermediary policy that accepts even more complex,
            abstract,
            and sparse type of advice
        </li>
        <p>OR</p>
        <li>by distilling to no advice, taking advantage of the already existing policies.</li>
    </ol>
    </p>

    <p>Even though the agent collects </p>
    <p> D = { (s<sub>0</sub>, a<sub>0</sub>, c<sub>0</sub>, r<sub>1</sub>),
        (s<sub>1</sub>, a<sub>1</sub>,c<sub>1</sub>, r<sub>2</sub>),···,
        (s<sub>H-1</sub>,a<sub>H-1</sub>, c<sub>H-1</sub>,r<sub>H</sub>)}<sub>j=1</sub><sup>N</sup>.</p>

    <p> the agent optimizes:</p>
    <p>
        max<sub>θ</sub> <b>E</b><sub>(s<sub>t</sub>, a<sub>t</sub>, τ)<sub>t</sub>∼D(&centerdot;|s<sub>t</sub>, τ)</sub>
        [log π<sub>θ</sub>(a<sub>t</sub>|s<sub>t</sub>, τ)]</p>
    <p>thus eliminating the coaching functions.</p>


    <p>The advantage of advice distillation over <i>behavioral cloning</i> is that the agent accepts a more sparse and
        abstract type of advice. This allows the agent to generalize better because the advice is invariant to internal
        distributions shifts of the agent.</p>

    <h3>Evaluation</h3>

    <p>During evaluation let the agent explore using π<sub>θ</sub> and compute the actual amount of reward the
        environment provides. </p>


    <h1>Experimental setup</h1>
    <p>Text for paragraph 3</p>

    <h1>Results and Discussion</h1>
    <p>Text for paragraph 3</p>

    <h1>Conclusion</h1>
    <p>Text for paragraph 3</p>


</dt-article>

<dt-appendix>
</dt-appendix>
http://www.incompleteideas.net/book/RLbook2020.pdf

<script type="text/bibliography">
  @article{watkins2023,
    title={Teachable Reinforcement Learning via Advice Distillation},
    author={Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, Abhishek Gupta},
    journal={arXivreprint 	arXiv:2203.11197},
    year={2023},
    url={https://arxiv.org/pdf/2203.11197.pdf}
  }
@book{suttonbarto2020,
    title={Reinforcement Learning},
    author={Richard S. Sutton, Anrew G. Barto},
    publisher={The MIT Press},
    year={2020},
    url={http://www.incompleteideas.net/book/RLbook2020.pdf},
    isbn={ISBN 9780262039246}
  }
  @article{mnih2013,
    title={Playing Atari with Deep Reinforcement Learning},
    author={Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller},
    journal={arXivreprint arXiv:1312.5602},
    year={2013},
    url={https://arxiv.org/pdf/1312.5602.pdf}
  }
  @article{dsilver2017,
    title={Mastering the game of Go without human knowledge},
    author={David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,
    Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel & Demis Hassabis},
    journal={Nature 550, 354–359},
    year={2017},
    url={https://www.nature.com/articles/nature24270}
  }
  @article{hessel2017,
    title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
    author={Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver},
    journal={arXivreprint arXiv:1710.02298},
    year={2017},
    url={https://arxiv.org/pdf/1710.02298.pdf}
  }
  @article{lynn2019,
    title={How humans learn and represent networks},
    author={Christopher W. Lynn, Danielle S. Bassett},
    journal={arXivreprint arXiv:1909.07186},
    year={2019},
    url={https://arxiv.org/pdf/1909.07186.pdf}
  }
  @article{barto90,
    title={On the Computational Economics of Reinforcement Learning},
    author={Andrew G. Barto, Santinder Pal Singh},
    journal={Proceedings of the 1990 Summer School},
    year={1990},
    url={https://web.eecs.umich.edu/~baveja/Papers/summerschool.pdf}
  }
  @article{korteling2021,
    title={Human- versus Artificial Intelligence},
    author={E. Korteling, G. C. van de Boer-Visschedijk, R. A. M. Blankendaal, R. C. Boonekamp, A. R. Eikelboom},
    journal={Frontiers in Artificial Intelligence},
    year={2021},
    url={https://www.frontiersin.org/articles/10.3389/frai.2021.622364/pdf}
  }
  @article{lyoms2007,
    title={The hidden structure of overimitation},
    author={Derek E Lyons 1, Andrew G Young, Frank C Keil},
    journal={PubMed PMID: 18056814 PMCID: PMC2148370},
    year={2007},
    url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148370/pdf/zpq19751.pdf}
  }
  @article{chopra2019,
    title={The first crank of the cultural ratchet: Learning and transmitting concepts through language},
    author={Sahila Chopra, Michael Henry Tessler, Noah D. Goodman},
    journal={CogSci},
    year={2019},
    url={https://www.semanticscholar.org/paper/The-first-crank-of-the-cultural-ratchet%3A-Learning-Chopra-Tessler/68303e377b6999f5634e71e7c1bd709c10fcef33}
  }
  @article{juarez2016,
    title={The Role of Dopamine and Its Dysfunction as a Consequence of Oxidative Stress},
    author={Hugo Juárez Olguín, David Calderón Guzmán, Ernestina Hernández García, Gerardo Barragán Mejía},
    journal={Oxidative Medicine and Cellular Longevity Volume 2016, Article ID 9730467},
    year={2016},
    url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684895/pdf/OMCL2016-9730467.pdf}
  }
  @article{bayer2005,
    title={Midbrain dopamine neurons encode a quantitative reward prediction error signal},
    author={Hannah M Bayer, Paul W Glimcher},
    journal={Pub Med Neuron PMID: 15996553 PMCID: PMC1564381},
    year={2005},
    url={https://pubmed.ncbi.nlm.nih.gov/15996553/}
  }
  @article{schultz1997,
    title={A Neural Substrate of Prediction and Reward},
    author={W. Schultz, P. Dayan, P. R. Montague},
    journal={Science 14 Mar 1997 Vol 275, Issue 5306 pp. 1593-1599},
    year={1997},
    url={https://www.science.org/doi/abs/10.1126/science.275.5306.1593}
  }
  @article{munos2016,
    title={Safe and efficient off-policy reinforcement learning},
    author={Munos, R., Stepleton, T., Harutyunyan, A., Bellemare,M},
    journal={Advances in Neural Information Processing Systems, pp. 1054–1062},
    year={2016},
    url={https://proceedings.neurips.cc/paper/2016/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf}
  }
  @article{precup2001,
    title={Off-policy temporal-difference learning with function approximation},
    author={Munos, R., Stepleton, T., Harutyunyan, A., Bellemare,M},
    journal={International Conference on Machine Learning, pp. 417–424},
    year={2001},
    url={http://incompleteideas.net/papers/PSD-01.pdf}
  }
  @article{arumugam2019,
    title={Deep Reinforcement Learning from Policy-Dependent Human Feedback},
    author={D. Arumugam, J. K. Lee, S. Saskin, M. L. Littman},
    journal={arXivreprint arXiv:1902.04257},
    year={2019},
    url={https://arxiv.org/pdf/1902.04257.pdf}
  }
  @article{macglashan2017,
    title={Interactive Learning from Policy-Dependent Human Feedback},
    author={James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David Roberts, Matthew E. Taylor, Michael L. Littman},
    journal={arXivreprint arXiv:1701.06049},
    year={2017},
    url={https://arxiv.org/pdf/1701.06049.pdf}
  }
  @article{morgan2015,
    title={Experimental evidence for the co-evolution of hominin tool-making teaching and language},
    author={T. J. H. Morgan, N. T. Uomini, L. E. Rendell, L. Chouinard-Thuly, S. E. Street, H. M. Lewis, C. P. Cross, C. Evans, R. Kearney, I. de la Torre, A. Whiten & K. N. Laland},
    journal={Nat Commun 6, 6029},
    year={2015},
    url={https://www.nature.com/articles/ncomms7029.pdf}
  }
  @article{waxman1995,
    title={Words as invitations to form categories: evidence from 12- to 13-month-old infants},
    author={S R Waxman, D B Markow},
    journal={Cogn Psychol. doi: 10.1006/cogp.1995.1016.},
    year={1995},
    url={https://www.sciencedirect.com/science/article/abs/pii/S001002858571016X}
  }
  @article{hussein2017,
    title={Imitation Learning: A Survey of Learning Methods},
    author={A. Hussein, M. Medhat Gaber, E. Elyan, C. Jayne},
    journal={ACM Computing Surveys Volume 50 Issue 2 Article No.: 21 pp 1–35},
    year={3017},
    url={https://dl.acm.org/doi/10.1145/3054912}
  }
  @article{ziebart2008,
    title={Maximum entropy inverse reinforcement learning},
    author={B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey},
    journal={AAAI},
    year={2008},
    url={http://ai.stanford.edu/~amaas/papers/amaas_aaai.pdf}
  }
  @article{ross2010,
    title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
    author={S. Ross, G. J. Gordon, J. A. Bagnell},
    journal={arXivreprint arXiv:1011.0686},
    year={2010},
    url={https://arxiv.org/pdf/1011.0686.pdf}
  }
  @article{christiano2023,
    title={Deep reinforcement learning from human preferences},
    author={P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, D. Amodei},
    journal={arXivreprint 	arXiv:1706.03741},
    year={2023},
    url={https://arxiv.org/pdf/1706.03741.pdf}
  }
  @article{knox2008,
    title={TAMER: Training an Agent Manually via Evaluative Reinforcement},
    author={W. B. Knox, P. Stone},
    journal={IEEE 7th International Conference on Development and Learning},
    year={2008},
    url={https://www.cs.utexas.edu/~ai-lab/pubs/ICDL08-knox.pdf}
  }
  @article{,
    title={},
    author={},
    journal={},
    year={},
    url={}
  }
  @article{,
    title={},
    author={},
    journal={},
    year={},
    url={}
  }



















</script>