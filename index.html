<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/front-matter">
  title: "Teachable Reinforcement Learning via Advice Distillation"
  description: "Description of the post"
  authors:
  - Claire, Sturgil:
  - Mihai, Dumitrescu: https://github.com/midumitrescu
  affiliations:
  - Google Brain: http://g.co/brain
  - BCCN Berlin: https://www.bccn-berlin.de/







</script>

<dt-article>
    <h1>Teachable Reinforcement Learning via Advice Distillation</h1>
    <h2>A description of the article</h2>
    <dt-byline></dt-byline>

    <h1>Abstract 1</h1>
    <p>This is the first paragraph of the article.</p>
    <p>We can also cite
        <dt-cite key="gregor2015draw"></dt-cite>
        external publications.
    </p>

    <h1>Introduction</h1>

    <p>Reinforcement Learning
        <dt-cite key="suttonbarto96!?"/>
        is a technique for allowing agents to explore their environments
        and learn strategies for maximizing the amount of reward they collect from the environment.
    </p>

    TODO: explain what safe rl is and make it clear that this paper is not really about safe rl. A discussion on how
    this could
    be made to work in <b>safe</b> context is at the end

    TODO: explain normally how RL agents learn: by random exploration, Q Learning & Function approximation for large
    problem setup.
    Explain what are samples.

    <p>While RL is innovative in the approach, it has the downside that it is very sample heavy
        <dt-cite key="sampleheavy!?"/>
        .
        TODO: explain what heady is. Show examples
        Usually, the more complex the problem the higher number of samples an agent requires learning from.
        There were approaches to making an agent learn faster
        <dt-cite key="learngaster!?"/>
        and or smarter
        <dt-cite key="learnsmarter!?"/>
    </p>

    <p>This is in stark contrast to how humans learn, though. Humans learn by imitating what their peers are doing
        <dt-cite key="paper_cit_9"/>
        or by being taught, typically by more expert peers (e.g. going to school, university or having a coach).
        During the learning process, students are receiving constant feedback from their peers to make sure their
        strategies for
        solving tasks at hand are appropriate.
        The experts also usually use a complex stepwise strategy for teaching. The student starts with some very basic
        training and gets
        introduced to more complex tasks after he has a pretty good understanding on how to solve easier tasks.
    </p>
    TODO: talk about dopamine

    <p>Humans typically learn faster and require fewer samples.</p>

    <p>The paper suggests solving issue of high number of samples by enabling agents to follow a similar learning
        strategy.
        More concretely, agents are made teachable. They learn how to accept instructions from humans that act as their
        teachers.
        The teachers are giving instructions to the agent on how to solve intermediary steps of a task and are not
        allowed to directly
        control the agent's movements. The paper calls these instructions <b>advice</b></p>

    <p>Similarly to the stepwise school curriculum of humans, the agents are trained on various levels of complexity of
        this <b>advice</b>.
        The paper suggests 3 steps of learning:
    <ol>
        <li><b>Grounding</b> - teaching the agent how to follow, simple, <i>low level <b>advice</b></i></li>
        <li><b>Improvement to higher level advice</b> - teaching the grounded agent to follow more complex, <i>higher
            level <b>advice</b></i></li>
        <li><b>Improvement to advice independence</b> - removing the teacher completely and allow the agent to interact
            with its environment independently
        </li>
    </ol>
    </p>

    <p>After learning, goes through a typical <b>evaluation</b> phase to test its performance.</p>

    <p>The paper makes the claim that it [proposes a framework for training automated agents using similarly
        rich interactive supervision] that we do not regard as being true. The advice implemented in the codebase not
        rich at all,
        see @Method. We will discuss in the before last paragraph a suggestion on how to extend this to a more rich
        language.
    </p>
    TODO Talk about requiring less samples and also requiring less complex environments for learning.

    <p>This is achieved via augmenting the reward signal typical in RL setting. The teacher has the ability to present a
        reward to the agent depending on how well it is following the given advice.</p>

    <p>To understand how this work, we will
        present the <b>Coaching-Augmented Markov Decision Process</b> formalism in the next section. We will then
        explain how
        this formalism is used to leverage this tiered structure of learning using <b>Off-policy Learning</b>
        <dt-cite key="off-policy-learning!?"/>
        We will then present our contribution to how we made the algorithm make use of <b>On policy Learning</b>
        <dt-cite key="on-policy-learning!?"/>
        .
        We will present some preliminary results, talk about the challenges we faced and then open up a discussions
        section.
    </p>

    TODO: Paper suggests tackling this issues by making the agent teachable. This means, make possible for humans to
    interact in
    an indirect way with the agent, by providing instructions that the agent should be able to learn.

    <p nolongerrelevant>The original paper we investigated
        <dt-cite key="teachablerrl!?"/>
        suggests using human interaction in a similar way
        that humans are coached. A <b>teacher</b> could be used to provide instructions called <b>advice</b> to the
        agent.

        Idea:
        1. agent becomes low level advice which has 2 purposes.
        1. Makes the agent follow the teacher's instruction
        2. However, this is done in a reduced environment, reducing the number of samples that the agent requires for
        learning
        2. After the agent can follow this low level advice, higher level of advice is provided, increasing the
        complexity of
        1. Instructions the agent can follow
        2. problems the agent can solve

    </p>
    <p nolongerrelevant>
        This general approach with the advantage that the agent can take advantage of already existing knowledge
        hopefully speeding up the learning process.
    </p>
    <p nolongerrelevant>Having a <b>human in the loop</b>raises the issue of cost TODO: improve. </p>


    <h1>Background</h1>
    <h2>Markov Decision Processes</h2>
    <p>RL typically works by implementing the <b>Markov Decision Process</b> formalism. The MDP is defined as a tuple
        {S, A, T, R, ρ, γ, p} where
    <ol>
        <li>S is the <i>state space</i> and represents valid positions where the agent could be found at any time</li>
        <li>A(s) is the <i>action space</i> and represents the valid actions that an agent can take being in a
            particular state
        </li>
        <li>T(s<sub>t</sub>, a, s<sub>t+1</sub>) is the <i>transition dynamic</i> and represents the probability of
            arriving at
            s<sub>t+1</sub> if at time t the agent was at s<sub>t</sub> and executing action a
        </li>
        <li>R(s, a) is the <i>reward</i> that an agent receives being and state s and executing action a</li>
        <li>ρ(s<sub>0</sub>) is the <i>initial state distribution</i>representing where the agent starts each episode
        </li>
        <li>γ is the <i>discount factor</i> balancing how important future rewards vs immediate ones are</li>
        <li>p(τ) is the <i>distribution over tasks</i> i.e. what kind of task the agent is supposed to solve</li>
    </ol>
    </p>

    <p>The agent decides on an action to take at each time step <i>t</i>. A set of decisions the agent takes is called a
        <b>policy</b> and is typically denoted by <b>π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ)</b>. The policy is
        called
        <b>θ</b> and is usually implemented by a probability distribution on the set <b>A</b>.

        The agent thus interacts with the environment and collects <b>trajectories</b> of the shape
        D = {s<sub>0</sub>,a<sub>0</sub>,r<sub>1</sub>,s<sub>1</sub>,a<sub>1</sub>,r<sub>2</sub>,···
        ,s<sub>H-1</sub>,a<sub>H-1</sub>,r<sub>H</sub>}<sub>j=1</sub><sup>N</sup>.
    </p>

    <h3>Solving the <b>MDP</b></h3>
    <p>The objective of a multi task <b>MDP</b> is to find the <b>policy θ<i></i></b> that maximizes the amount of
        future discounted rewards. Formally, it looks for</p>
    <p>

        max <sub>θ</sub>[<b>E</b><sub>a<sub>t</sub>∼π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ)</sub>(∑<sup>∞</sup><sub>t=0</sub>
        γ<sup>t</sup>r(s<sub>t</sub>, a<sub>t</sub>, τ)>)]

    </p>

    <p>
        where <b>E(</b>X<b>)</b>=&lt;X&gt; represents the expected value the random variable X.
    </p>

    <h3>Exploration/exploitation dilemma</h3>

    <p>Typically agents need to execute random actions to discover trajectories which prove to be of high reward. In
        case such
        are found, the agent increases the probability of taking similar actions in the future. Because of high
        dimensional <b>state x action</b> space,
        the agent typically needs to try out a lot of combinations to make sure it found the best one. The agent always
        needs a balance
        between trying out random new actions and commit to already known high reward ones. It is still an unsolved
        problem
        to find this optimal balance. This is called the <b>exploration/exploitation dilemma</b> agents typically face
        and quickly explains the need for many samples (#introduction).
    </p>


    <h2>Coaching-Augmented Markov Decision Processes</h2>

    <p>The paper extends the classical <b>MDP</b> by providing two extensions:
    <ol>
        <li>C = {c<sub>t</sub>}, the set of <i>coaching functions</i>
            where c<sub>t</sub> represents advice given to the agent at time <i>t</i>.
        </li>
        <li>R<sub>CMDP</sub>=R(s,a) + R<sub>coach</sub>(s,a), where R(s,a) is the previous reward presented by the
            environment and
            R<sub>coach</sub>(s,a) represents the added reward the coach provides if the agent follows his advice
        </li>
    </ol>
    </p>

    <p><br/>c<sub>j</sub> used in the paper is either:
    <ol>
        <li>Directional Advice <i>(Up (0,1), Down(0, -1), Left(-1, 0) or Right(1, 0)</i></li>
        <li>Cardinal Waypoint Advice <i>(e.g. Go To (3,1))</i></li>
    </ol>
    </p>

    <p>but could be extended to include natural language richer type of advice (see #discussions)</p>

    <p>

        Like this, we formally define the <b>Coaching Augmented MDP (CAMP)</b> = {S, A, T, R<sub>CMDP</sub>, ρ, γ, p, C}
    </p>

    <p>
        The new optimization problem is to find the <i>best</i> policy <b>θ</b> that maximizes rewards from <b>both</b>
        the environment
        and the coaching functions i.e.
    </p>
    <p>max <sub>θ</sub>[<b>E</b><sub>a<sub>t</sub>∼π<sub>θ</sub>(&centerdot;|s<sub>t</sub>, τ,
        c<sub>t</sub>)</sub>(∑<sup>∞</sup><sub>t=0</sub> γ<sup>t</sup>r(s<sub>t</sub>, a<sub>t</sub>, τ)>)]</p>

    <p>representing an agent that interacts with the environment and has access to advice presented under the form of
        the coaching function c<sub>t</sub>
    </p>

    <p>The big advantage of <b>CMDP</b> to plain <b>MDP</b> is that it formalizes the interaction of the agent with
        a <i>human in the loop trainer</i>. The agent learns that <i>following human instructions/advice provides
            reward</i> and
        it starts doing so.
        The agent can now take advantage of <i>expert knowledge</i>.
        TODO: this could be improved
    </p>

    <h1>Method</h1>
    <p>Our target is to quickly train agents that are able to solve complex tasks.
        Considering the #Exploration/exploitation dilemma, we would want agents that quickly find high reward
        policies eliminating a lot of random exploration.</p>
    <p>The paper suggests a tier based teaching scheme, speeding up learning
        versus typical <b>MDP</b>.
    </p>.

    <p>
        This is done by:
    <ol>
        <li>making the agent follow the coaching it receives</li>
        <li>introducing increasingly complex coaching</li>
        <li>guiding the agent to the goal</li>
        <li>allowing him to quickly understand that specific <i>policies</i>provide <b> high reward</b></li>
        <li>eliminate the coaching</li>
        <li>allow the agent to follow the already found <b>high reward</b> policies</li>
    </ol>

    <p> The paper distinguisly introduces the following phases:
    <ol>
        <li><b>Grounding</b> - with the focus of making the agent interpret and follow <i>low level, simple</i> <b>advice</b>
        </li>
        <li><b>Improvement</b>, which is of two types:
            <ol>
                <li>from one type of <b>advice</b> to another type of <b>advice</b> - typically from <i>low level,
                    simple</i> <b>advice</b> to
                    <i>high level, more complex</i> <b>advice</b>
                </li>
                <li>from one type of <b>advice</b> to <b>no</b> <b>advice</b> - allowing the agent to figure out policies
                    that allow him to decide independently on next actions
                </li>
            </ol>
        </li>
        <li><b>Evaluation</b> - which represents the phase in which the agent does not learn anymore and the already
            learned policy is evaluated</li>
    </ol>

    </p>
    <h4>Grounding</h4>
    <p>The main objective of grounding is to make the agent follow/interpret the provided <b>advice</b>.
        The big advantage vs. plain <b>MDP</b> solving tasks is that the agent can be trained on a <i>very simple</i>
        environment. The trajectories can be chosen a lot simpler than the ones in a complex environment, where the agent
        must follow many steps to reach a goal (e.g. a game or a maze).
    </p>
    <p>Theoretically the advice in the grounding phase can be anything. Chosen wisely it can support the idea of tiered
    learning. Therefore, <i>the grounding</i> phase is the candidate for the simplest available advice i.e.
        <b>Directional Advice</b>
        At every time step, the agent is rewarded with the dot product between the advised direction and the action it took.
        E.g. Should the agent be advised to move up (i.e. Direction (0, 1)) and he moves in direction (0, 0.5) he will
        be rewarded with <(0, 1) * (0, 0.5)> = 0.5.<br/>
        Should he move in direction (1, -0.5) i.e. diagonally down, he will receive a negative reward of
        <(0, 1) * (1, -0.5)> = - 0.5
    </p>

    <p>By apply</p>

    <h1>Experimental setup</h1>
    <p>Text for paragraph 3</p>

    <h1>Results and Discussion</h1>
    <p>Text for paragraph 3</p>

    <h1>Conclusion</h1>
    <p>Text for paragraph 3</p>


</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }







</script>